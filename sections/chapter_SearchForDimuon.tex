\chapter{The Resonance Search on the Dimuon Signature}
\label{chapter:dimuon}

\epigraph{\textit{But the truth can be re-found; most often it has already been written elsewhere. .}}{--Jacques Lacan, Ecrits}
%Lacan, J. (2006). The function and field of speech and language in psychoanalysis. Ã‰crits. (Trans. B. Fink). New York, NY: Norton.

\section{Introduction}

    % Why this is interesting  
    Owing to its unprecedented high energy, the run I and early run II of ATLAS has long focused on the high mass resonances searches. This has left some gaps in the low mass region that are unexplored. In the two lepton final state, there has been an analysis after the Z boson peak since the beginning of Run 1, however the mass region below the Z mass peak has left a region totally uncovered on ATLAS.
    
    % Why this signature is something good to be looked for.
    Since ATLAS collide partons, there are fewer background on leptons than jet processes, this allows lower transverse momentum lepton events to be saved. Using two muons as the resonance final state items, lower mass resonances can be searched for, going below the limits of the dijet resonances. The signal significance would be higher compared to jet. 

    % Why this result is intersting to look for
    This search results is interesting to the theoretical community can be reinterpreted in the dark matter summary, as another way to look for the dark matter mediator particle, as well as other theory models, such as the $W^{'}$ and $Z'_{SSM}$ , more details are described in the theory session. 

    % Gaussian Process method for the background 
    The analysis utilizes gaussian process as a background estimation method, it's a method that is. 

\section{Historical searches}    
    Previous searches has been done directly in Tevatron experiment. Indirect constraints has been made in LEP as well. 

    CMS results and tevatron results: 
    
    This study will be an independent finding from ATLAS and the first search done in this region.  



\section{Theoretical Model}
There are several models that motivates the search in the low mass dilepton regions, they are listed below along with their motivations:

\subsection{A Kinetically Mixed Z'}

By adding an additional $B^{'}_{\mu\nu}$ field strength tensor for a new U(1)' field, the Lagrangian can be represented as such:

\begin{equation}
    $$ \mathcal{L} = \mathcal{L}$_{SM}$ - $\frac{1}{4}$ B^{'}_{\mu\nu}B^{'\mu\nu} + \frac{\epsilon}{2} B_{\mu\nu}B^{'\mu\nu} + \frac{M^{2}_{B'}}{2} B'_{\mu}B'^{\mu} $$
\label{eq:darkphoton}
\end{equation}

The mass can also be generated by a kinematic mixing between the fields of a gauge dark field $Z_{D,0}$ and the hypercharge field B, the particle can be generated by via mixing with the Z or the photon~\ref{dilepton2014}.
The dark photon can be generated this way. 

%\subsection{Dark Higgs}
%Following from ~\ref{eq:darkphoton}, the mass of $M_{B'}$ can be obtained from the vacuum expectation value of a singlet scalar field charged by the new U(1)', the dark higgs field. The dark higgs can also couple to leptons and thus be looked for via the dilepton resonance signature.

\subsection{LHC Dark matter Benchmark}
The dark matter benchmark from the LHC uses the effective field theory approach. (See section for more detail on this appraoch) It uses contact interaction operator as the basis of the theory model building blocks. This way, the model can be reduced to a handful of experimentally measurable qualities for more complete theory model interpretations. 
In the dark matter benchmark of the LHC, it extends the Standard Model by an additional U(1) symmetry, and assumed dark matter along with some Stanard Model particles are all charged under this. A new gauge boson can thereby facilitate interaction between the standard model and dark matter field. 

The dark matter mediator can either be an axial vector or a vector, the corresponding lagrangians are as such:

\[ \mathcal{L}_{vector}= g_{q} \sum$_{q=u,d,s,c,b,t}$ Z'_{\mu}\bar{q}\gamma^{\mu}q + g_{\chi}Z'_{\mu}\bar{\chi}\gamma^{\mu}\chi \]


\[ \mathcal{L}_{axial vector}= g_{q} \sum$_{q=u,d,s,c,b,t}$ Z'_{\mu}\bar{q}\gamma^{\mu}\gamma^{5}q + g_{\chi}Z'_{\mu}\bar{\chi}\gamma^{\mu}\gamma^{5}\chi \]

This is also applicable to leptons, under this calculation the minimal parameters of the model is reduced to {g_{q}, g_{\chi}, m_{\chi}, M_{med}}. 

Similar arguments can be made for the lepton couplings. 

These models are generated in the event generator with NLO+PS accuracy with the POWHEG generator, the product goes through parton showering at PYTHIA8 with detector simulation from GEANT4. 

In this study, only the coupling to leptons are used, 

\subsection{Other models}
There are also other theories including the axion models~\ref{dilepton2017} and those on quantum blackholes, W' and other models that predicts dimuon resonances in this mass region. 

The following search will be focused on the search on the vector dilepton signature from the dark photon model and the dark matter benchmark. But as gaussian limits are also set, the results will be reinterpretable for many other models that predicts scalar or axion-vector resonances. 


% Email Tim about problems regarding the dimuon resonance signature.

\section{Experimantal Signature}

The analysis uses the resonance finding signature, it adds the 4-vector of the two candidate muons and reconstruct a resonance candidate. Following Einstein's Special Relativitiy relations:

Resonance finding is a well known strategy, many particles has been discovered that way before, which include the W, Z and the Higgs boson.
The strategy relies on a smooth background and the search of a signal as a bump on top of that. The underlying assumption is that the background can be well-estimated, and the signal width is smaller than that of the background. 

Using the ATLAS proton-proton dataset, in the dimuon mass region of (12-75GeV) the standard model background is mostly dominated by the Z boson tail and some ttV decay, the background is predicted to be smooth, and thus can be predicted by either a smooth function or other data driven method despite the lack of MC. A resonance signal, if it has a width smaller than that of the background, will show up as a bump on the smoooth spectrum and can be searched for as such.

\section{The background fit}

\subsection{The signal injection test}

\subsection{The spurious signal test}

\section{Data Preparation}
This analysis uses the ATLAS 13 TeV proton-proton collision dataset generated between 2014-2019. The dataset goes through several steps of data preparation before getting the dimuon mass data spectrum for resonance search analysis. 
In the following section, the steps will be studied over
First, the trigger was analyzed, then the event is selected with cuts applied for the optimal sensitivity. 
The data preparation is done by the SUSYtool framework. 

\subsection{Trigger}
Due to the large amount of data and limited computing bandwidth, in ATLAS, the data goes through a triggering procedure, where it was decided in real time whether the the collision event should be kept. An event passes the trigger and gets kept if it contains at least one or more online object that satisfy the trigger requirement, the event is called to be "kept in a stream" named by the requirements.

The analysis uses unprescaled trigger stream~\footnote{} through an "AND" trigger of the different streams to maximize events kept for the analysis. Pre-scaled triggers~\footnote{} and delayed streams~\footnote{} are not used in this analysis. 

The following table summarized the triggers map used in this analysis, the integrated luminosity shows the effective amount of data available
These triggers used are all fully efficient, meaning cuts are taken from $p_{T}$ values where 99.5\% events passes the trigger.

No delayed stream or overlap stream are used in this analysis. 

A different set of trigger strategies has been studied and they can be found in the following part: 

\subsubsection{trigger weighting}


\subsection{Event Selection}
The trigger provide the first cut into the dataset by requiring muons pairs above a certain transverse momentum to be chosen. Further selection criteria are applied as the following. 

\subsection*{Good Run List}
Due to different running condition of different detector parts during the time of the run, not all recorded data that passes the trigger are of the quality worthy to be analyzed. The ATLAS Data Quality group has published data quality basing on the health of various subsystem during runs. 

Different event selection cuts are applied to the events to select objects that are fully optimized for this particular analysis.  

%In addition, as different events utilizes different subsystem, the data quality flag is also specific to the physics object being analysed. 
In addition, as part of the data integrity checks, events that are considered corrupted/bad with LAr burst or tile corruption are also rejected. 


%\subsection{Data Integrity Checks}



\subsection{Simulated Physics Events(Monte Carlo)}
The analysis is data driven, meaning that the background estimation is done directly on the data itself along with signal search. However, simulated physics events, or Monte Carlo, is still used to study the different background estimation strategies and their corresponding signal sensitivity, as well as the detector resolutions and binning. 
Using a parton-distribution function, an event generator is first used to simulate pp colision and their tree level outcomes. The parton showers are then simulated probabilistically through some hadronization models, the detector response to these events are then modeled by a GEANT4 detector simulator. 
In this analysis, as there isn't enough Monte Carlo for the background estimation, fast simulation is also used to avoid the problem from a low statistics, the fast simulation is generated from pythia and a smearing function produced afterwards. 

%and therefore different weighting are used to make sure that theMonte Carlo matches up to the data events being studied. 


\subsection{Object Preparation}
The analysis uses muons as objects, their detailed preparation is shown in chapter~\ref{dataprep:muon}. 

Here a list of the different calibration working points chosen for the analysis.


\subsection{Dimuon Mass Spectrum Resolution}
The dimuon mass resolution is found through a width-to-mean(\frac{\mu}{sigma}) Gaussian fit on the $m^{reco}/m^{truth}$ on the four-vector addition of the leading and subleading muons. The truth muons came from PYTHIA generation whereas the reco value came from the GEANT4 detector reconstruction. A Gaussian fit is performed to find the mean ($\mu$) and the width($\sigma$). The result is found as the following: 

\subsection{Binning Strategy}
To prepare a dimuon mass spectrum, the data must be properly binned. The bins has to be narrower than the the width of the expected signal, as this would minimize the possibility of mistaking fluctuation as an excess and improve characterization of the signal. A uniform binning is chosen, as it would make the background estimation more simplified.

A fit between the detector resolution and the signal width is done here. 
The binning value is chosen to be 0.25 GeV, the experimental resolution of the middle of the data spectrum. All the signal from the theory model will take up more than at least two bins.
% Remember argument for choosing this?

\subsubsection{MC/Data Comparison}
After the MC are properly prepared and weighted, it is then compared to data to see if a proper comparison is seen. 

\subsection{The Data spectrum}
    \subsubsection{Background Modelling}
        The fitting method is performed by gaussian process and the fit is evaluated in the following way. 
        A Wilk test can be done to the fit. 
        \subsubsubsection{Gaussian Process} 
        \subsubsubsection{Signal Injection Test}
        \subsubsubsection{Spurious Signal Test}

 
\section{Statistics Testing}
In this section, the statsitical testing will be presented 
The statistics testing on the model can be presented as a test on the significance Z: 

\begin{equation}
\[ Z= \Phi^{-1}(1-p) \]
\end{equation}

,here $\Phi_{-1}$, p is a numeric representation of the agreement between the data and the model, whereas the $\Phi$ is its conversion in terms of quantile of on a Gaussian distribution that is the cummulative probability. From here on, Z will be called the significance and p will be referred to as the p-value. 

There are two distinct statistics test that are done here, each test done on a threshold value of Z. Here, the null hypothesis is the hypothesis that include only known processes from the Standard Model hypothesis with no excess; the alternative hypotheis is the hypothesis that include both background and signal of a certain signal strength.

1.  The Search: Rejection of the null hypothesis
This step is sometimes also referred to as the discovery test. This test is done on the null hypothesis alone. If the given data return a the Z > 5, the null hypothsis is rejected, something beyond the Standard Model is observed in the data to a statistically significant degree. Note that however the rejection test is a necessary condition but not a sufficient condition for discovery. For example, in the Higgs boson analysis of 2012, a significance of Z> 5 was observed in the search, but more test
were required other than this test before discovery could be claimed. Z =5 corresponds to p = 2.87 \cdot 10^{-7}.
In the Bayesian method, this is a separate step that is called the search phase, whereas in the frequentist method, this is combined with the limit setting of the likelihood ratio. 

2. The Upper limit setting
The upper limit setting is the finding of the signal strength where the alternative hypothesis is rejected at 95\% confidence level. The 95\% confidence level corresponds to a p-value of 0.05, and a Z value of 1.64. The signal strength can also be represented as the cross section. 
In the Bayesian statistics bethod, this is often only done when test 1 failed (when the null hypotheis of standard model is not rejected, or no excess is observed). In the frequentist method test, by using the likelihood ratio as the test statistics to calculate the p value, both of these tests can be done in the limit setting. 

In the following section, a Bayesian based method based on the BATTools, and a frequentist method based on the Asimov calculation will also be discussed. 
the Bayesian tool is used for the dijetISR analyses, whereas the frequentist method is used for the dimuon analysis. 


\section{The Bayesian Search Phase}
    The search is the phase in the analysis where it's studied whether the a discrepancy from the Standard Model background prediction is seen, if yes, an excess is claimed to be found and a through testing on the statsitics procedure will be done. Statistically this is defined by an rejection of a null hypothesis scenario.

    The statistics of the background distribution can be described by a probability distribution, as all other counting experiments, the this is described by a Poisson probability distribution. 

    \[ p(x|\lambda) = \frac{\lambda^{k}e^{-\lambda}}{k!} \]

    The test is done in a frequentist manner, it compares the observation outcome x with a fixed critical value $\alpha$ of a probability distribution generated from pseudo-experiment from a Poisson distribution. The value used to reject or accept the null hypothesis is a probability quantity called the p-value. Statistically, it is known to be a false-discovery probability. 
    If the observation compared with the distribution outcome is smaller than the critical p-value, the null hypothesis is rejected.  

    \[ P(x \ni w|H)<= \alpha \]

    As the test wants to see how much deviation is seen between the data and the prediction, the observable x is presented as a test statistics that quantifies deviation between the observation and predictable. The observable is then compared against the pseudoexperiment distribution in such a form. 
    
    \[ p = P(T>=t$_{0}$| H$_{0}$) \]


    \subsubsubsection*{Test Statistics}
    Different test statistics can be chosen to describe how well the data fits the background prediction,  
    Traditionally, how well the data fits the background estimation can be described by either the $\chi^{2}$ test or the log-likelihood that gets derived from the Poisson distribution of the 
    in the case of excess search, a test statistics chosen would need to fit the need of the signal shape searched for.
    However, these test statistics fail to take into account the excess from the neighboring bins, and therefore a different bumphunter statistics is defined, in the analysis below. 
    In each bumphunter search window(from bin m to n), the expected number of events in the window is: 

    \[ d= \sum_{i=m}^{n} d_i \]

    The Observed number of events is: 
    
    \[ b= \sum_{i=m}^{n} b_i \]
    
    Assuming the observed counts in each bin follows a Poissonian distribution, the cumulated count across all the bins is as the following:

    \begin{equation}
    \[ t= \Bigg\{  \sum_{n=0}^{d} \frac{b^{n}}{n!} e^{-b} for d < b \newline
        \sum_{n=d}^{\infinity} \frac{b^n}{n!} e^{-b} for b \leq d \]

    \label{test statistics}
    \end{equation}

    The above expression can be represented by the Gamma function: 

    \begin{equation}
    t = \Bigg\{  1-\gamma(d+1, b), for d < b \nextline
                 \gamma(d,b)       for b \leq d

    \label{test statistics}
    \end{equation}


    This quatity takes the overall deficit and or excess in the region compared to the \chi^{2} and loglikelihood calculation. Only excesses are used for this study due to the nature of the resonace signature being looked for. 

    In this analyses two kind of bump-hunter statistics are used, one is used to described the localized excess in each window and the other on whether an excess is found for the overall spectrum.

    Local excesses is represented by a set of bumphunter test statistics calculated for a window scan along the spectrum from 2 bin wide to half the spectrum size.

    An overall standard model agreement is defined by the negative log of the smallest p value calculated for any window scanned given the pseudo experiemntal. The test statstiscs can be represented as such 
    \[ t_{0} = - log t_{min} \]

    
    In addition to the bumphunter test statistics, there is the tailhunter, KS, Jeffreys and no sidebands bumphunter that can also be chosen for different signal searched for in other physics analyses. More information can be found in the Bumphunter paper here: . 


    \subsubsection{The Search Test: the Bumphunter}
    % Look up the bumphunter fit. 
    The search in the analysis is defined as the statistical process to look for an excess beyond the Standard Model null hypothesis using the bumphunter test statistics. 

    The procedure is describe bumphunting procedure to look for the search test.
    The bumphunter test statistics are performed all the windows along the spectrum starting two bin wide to half the spectrum size. This is assumed to be true for all of the following steps illustrated. 

    1.  First a fit is performed, according to the method from the last section.
    2.  A set of bumphunter window are defined across the spectrum. Bumphunter statistics and their p values are calculated. 
    If all window has a bumphunter statistics p-value above 0.01, stop, no excesses is found. Alternatively, if any one of the window goes below p< 0.01, the window most discrepant window (the window with the lowest p-value) is removed, and the fit is reperformed.
    3. After the fit is reperformed, the bumphunter statistics and the p value is recalculated for every window, if all p-value is above 0.01, stop, otherwise, see if the most discrepant region is adjacent to the fit, if yes, add the adjacent two bin (left and right) to the fit. 
    Three outcomes are possible, if there is one excluded window and a background fit with a bumphunter statistics p-value of >0.01, the null hypothesis is rejected, an excess may be seen; if no window is excluded and the fit has a bumphunter p-value statistics above 0.01, the null hypothesis is accepted; in all other sceanrios, more tests needs to be done.

    A summary of test on a background only test and a data spectrum with signal injected is shown here.

    
\section{Limit Setting}

The previous section performed a test on whether the null hypothesis of there is only known process in the Standard Model is rejected. In the section of limit setting, the alternative hypothesis is looked at, and an alternative hypothesis on the mass or cross section of the signal model the data has excluded the information of interest. 

The limit is a hypothesis testing procedure that test both the 
compares the null hypothesis of absence of signal ($H_{0}$) to the hypothesis where there is a signal($H_{1}$). 
The limit is a projection of the $H_0$ and $H_1$ hypothesis in either the cross-section or the signal strength axis. This provide another way to discuss signal discovery, and information on an 95\% confidence limit in the If $H_1$ shows agreement with $H_{0}$ when fluctuation is taken into account, the 
Even in the absence of a signal, a statement can be made about the signal strength value that is ruled out by the signal+ background. 

The limit setting procedure can be done in both the frequentist and Bayesian way on ATLAS, and in the analyses presented, the dimuon analysis is done in the frequentist way, whereas the dijet analysis is done in the Bayesian way. 

A test statistics is defined as the statistics that 



\subsection{Asimov frequentist limits}
The frequentist limit is done in the asimov way to save on computing resources for the calculation. 

The limit setting procedure is done by an Asimov method, 

\section{Systematics}
\section{Future Extensions}

